{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original code credit\n",
    "https://www.pyimagesearch.com/2018/09/17/opencv-ocr-and-text-recognition-with-tesseract/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(scores, geometry, min_confidence):\n",
    "    # grab the number of rows and columns from the scores volume, then\n",
    "    # initialize our set of bounding box rectangles and corresponding\n",
    "    # confidence scores\n",
    "    (numRows, numCols) = scores.shape[2:4]\n",
    "    rects = []\n",
    "    confidences = []\n",
    "    # loop over the number of rows\n",
    "    for y in range(0, numRows):\n",
    "        # extract the scores (probabilities), followed by the\n",
    "        # geometrical data used to derive potential bounding box\n",
    "        # coordinates that surround text\n",
    "        scoresData = scores[0, 0, y]\n",
    "        xData0 = geometry[0, 0, y]\n",
    "        xData1 = geometry[0, 1, y]\n",
    "        xData2 = geometry[0, 2, y]\n",
    "        xData3 = geometry[0, 3, y]\n",
    "        anglesData = geometry[0, 4, y]\n",
    "        # loop over the number of columns\n",
    "        for x in range(0, numCols):\n",
    "            # if our score does not have sufficient probability,\n",
    "            # ignore it\n",
    "            if scoresData[x] < min_confidence:\n",
    "                continue\n",
    "            # compute the offset factor as our resulting feature\n",
    "            # maps will be 4x smaller than the input image\n",
    "            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n",
    "            # extract the rotation angle for the prediction and\n",
    "            # then compute the sin and cosine\n",
    "            angle = anglesData[x]\n",
    "            cos = np.cos(angle)\n",
    "            sin = np.sin(angle)\n",
    "            # use the geometry volume to derive the width and height\n",
    "            # of the bounding box\n",
    "            h = xData0[x] + xData2[x]\n",
    "            w = xData1[x] + xData3[x]\n",
    "            # compute both the starting and ending (x, y)-coordinates\n",
    "            # for the text prediction bounding box\n",
    "            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n",
    "            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n",
    "            startX = int(endX - w)\n",
    "            startY = int(endY - h)\n",
    "            # add the bounding box coordinates and probability score\n",
    "            # to our respective lists\n",
    "            rects.append((startX, startY, endX, endY))\n",
    "            confidences.append(scoresData[x])\n",
    "    # return a tuple of the bounding boxes and associated confidences\n",
    "    return (rects, confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_context(context):\n",
    "    east = os.getenv('east')\n",
    "    net = cv2.dnn.readNet(east)\n",
    "    setattr(context.user_data, 'net', net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(context,event):\n",
    "    width=int(event.fields['width'])\n",
    "    height=int(event.fields['height'])\n",
    "    padding=float(event.fields['padding'])\n",
    "    min_confidence = float(event.fields['min_confidence'])\n",
    "\n",
    "    context.logger.info(event.headers)  \n",
    "    \n",
    "    img_raw = event.body\n",
    "    #open(\"/User/tmp/img.jpg\",\"wb\").write(event_data)\n",
    "    image = np.asarray(bytearray(img_raw), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    \n",
    "    orig = image.copy()\n",
    "    (origH, origW) = image.shape[:2]\n",
    "   \n",
    "\n",
    "    \n",
    "    # set the new width and height and then determine the ratio in change\n",
    "    # for both the width and height\n",
    "    (newW, newH) = (width, height)\n",
    "    rW = origW / float(newW)\n",
    "    rH = origH / float(newH)\n",
    "\n",
    "    # resize the image and grab the new image dimensions\n",
    "    image = cv2.resize(image, (newW, newH))\n",
    "    (H, W) = image.shape[:2]\n",
    "    print(\"NH %s NW%s\"% (H, W) )\n",
    "    # define the two output layer names for the EAST detector model that\n",
    "    # we are interested -- the first is the output probabilities and the\n",
    "    # second can be used to derive the bounding box coordinates of text\n",
    "    layerNames = [\n",
    "        \"feature_fusion/Conv_7/Sigmoid\",\n",
    "        \"feature_fusion/concat_3\"]\n",
    "\n",
    "    # load the pre-trained EAST text detector\n",
    "    net = context.user_data.net\n",
    "\n",
    "    \n",
    "    #ret, buffer = cv2.imencode('.jpg', image) \n",
    "    #open(\"/User/tmp/img.jpg\",'wb').write(buffer)\n",
    "    \n",
    "    # construct a blob from the image and then perform a forward pass of\n",
    "    # the model to obtain the two output layer sets\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (W, H),\n",
    "        (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    (scores, geometry) = net.forward(layerNames)\n",
    "\n",
    "    # decode the predictions, then  apply non-maxima suppression to\n",
    "    # suppress weak, overlapping bounding boxes\n",
    "    (rects, confidences) = decode_predictions(scores, geometry, min_confidence)\n",
    "    boxes = non_max_suppression(np.array(rects), probs=confidences)\n",
    "\n",
    "    # initialize the list of results\n",
    "    results = []\n",
    "\n",
    "    # loop over the bounding boxes\n",
    "    for (startX, startY, endX, endY) in boxes:\n",
    "        # scale the bounding box coordinates based on the respective\n",
    "        # ratios\n",
    "        startX = int(startX * rW)\n",
    "        startY = int(startY * rH)\n",
    "        endX = int(endX * rW)\n",
    "        endY = int(endY * rH)\n",
    "\n",
    "        # in order to obtain a better OCR of the text we can potentially\n",
    "        # apply a bit of padding surrounding the bounding box -- here we\n",
    "        # are computing the deltas in both the x and y directions\n",
    "        dX = int((endX - startX) * padding)\n",
    "        dY = int((endY - startY) * padding)\n",
    "\n",
    "        # apply padding to each side of the bounding box, respectively\n",
    "        startX = max(0, startX - dX)\n",
    "        startY = max(0, startY - dY)\n",
    "        endX = min(origW, endX + (dX * 2))\n",
    "        endY = min(origH, endY + (dY * 2))\n",
    "\n",
    "        # extract the actual padded ROI\n",
    "        roi = orig[startY:endY, startX:endX]\n",
    "\n",
    "        # in order to apply Tesseract v4 to OCR text we must supply\n",
    "        # (1) a language, (2) an OEM flag of 4, indicating that the we\n",
    "        # wish to use the LSTM neural net model for OCR, and finally\n",
    "        # (3) an OEM value, in this case, 7 which implies that we are\n",
    "        # treating the ROI as a single line of text\n",
    "        config = (\"-l eng --oem 1 --psm 7\")\n",
    "        text = pytesseract.image_to_string(roi, config=config)\n",
    "\n",
    "        # add the bounding box coordinates and OCR'd text to the list\n",
    "        # of results\n",
    "        results.append(((startX, startY, endX, endY), text))\n",
    "\n",
    "    # sort the results bounding box coordinates from top to bottom\n",
    "    results = sorted(results, key=lambda r:r[0][1])\n",
    "\n",
    "    # loop over the results\n",
    "    ocr_text=[]\n",
    "    for ((startX, startY, endX, endY), text) in results:\n",
    "        # display the text OCR'd by Tesseract\n",
    "        print(\"OCR TEXT\")\n",
    "        print(\"========\")\n",
    "        print(\"{}\\n\".format(text))\n",
    "        \n",
    "        ocr_text.append(\"{}\\n\".format(text))\n",
    "                \n",
    "\n",
    "\n",
    "        # strip out non-ASCII text so we can draw the text on the image\n",
    "        # using OpenCV, then draw the text and a bounding box surrounding\n",
    "        # the text region of the input image\n",
    "        \"\"\"\n",
    "        text = \"\".join([c if ord(c) < 128 else \"\" for c in text]).strip()\n",
    "        output = orig.copy()\n",
    "        cv2.rectangle(output, (startX, startY), (endX, endY),\n",
    "            (0, 0, 255), 2)\n",
    "        cv2.putText(output, text, (startX, startY - 20),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "        # show the output image\n",
    "        cv2.imshow(\"Text Detection\", output)\n",
    "        cv2.waitKey(0)\n",
    "        \"\"\"\n",
    "    #return context.Response(body=ocr_text,headers=None,status_code=201)\n",
    "    return ocr_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "# converts the notebook code to deployable function with configurations\n",
    "from mlrun import code_to_function, mount_v3io, mlconf\n",
    "import os\n",
    "\n",
    "fn = code_to_function('menu-ocr', kind='nuclio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/User/igzshare/cv/text_recognition/frozen_east_text_detection'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib,os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.function.RemoteRuntime at 0x7fb6b4415fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the API/trigger, attach the home dir to the function\n",
    "fn.with_http(workers=1).apply(mount_v3io())\n",
    "fn.spec.build.commands = ['pip install opencv-python v3io_frames requests pillow pytesseract imutils','apt -y update && apt -y install tesseract-ocr']\n",
    "fn.spec.build.base_image = 'mlrun/mlrun'\n",
    "fn.spec.min_replicas = 1\n",
    "fn.spec.max_replicas = 1\n",
    "fn.set_env('east',os.path.join(pathlib.Path().absolute(),'frozen_east_text_detection.pb' ))\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mlrun] 2020-07-29 19:05:50,875 deploy started\n",
      "[nuclio] 2020-07-29 19:05:50,899 project name not found created new (img-text)\n",
      "[nuclio] 2020-07-29 19:19:03,064 (info) Build complete\n",
      "[nuclio] 2020-07-29 19:19:09,135 (info) Function deploy complete\n",
      "[nuclio] 2020-07-29 19:19:09,141 done creating img-text-menu-ocr, function address: 3.129.97.78:31546\n"
     ]
    }
   ],
   "source": [
    "# nuclio: ignore\n",
    "addr = fn.deploy(project='img-text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['addr'] = addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"COFFEEJ\\n\", \"\\n\"]"
     ]
    }
   ],
   "source": [
    "!curl -XPOST --data-binary @\"storefronts/sb.jpg\" \"http${addr}/?width=320&height=320&padding=0.00&min_confidence=0.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-mlrun-0.4.10]",
   "language": "python",
   "name": "conda-env-.conda-mlrun-0.4.10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
